{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yush/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials \n",
    "spotify = spotipy.Spotify(client_credentials_manager=SpotifyClientCredentials(client_id='0c7c294438794162b901ce0554a38703', client_secret='cecd70a59c1344f7a7d8d7cbec4fe6bb'))\n",
    "\n",
    "#uncomment above and replace CLIENT_ID and CLIENT_SECRET with the id and secret you get from creating your app with spotify (going through the \"Getting Started\" instructions in the Spotify Web API documentation)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spotify Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/Discussions.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m discussions_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/Discussions.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m ratings_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/Ratings.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# creating new columns and intializing each value to 0\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sarth\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sarth\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sarth\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\sarth\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\sarth\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\sarth\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\sarth\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    866\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/Discussions.csv'"
     ]
    }
   ],
   "source": [
    "discussions_df = pd.read_csv('data/Discussions.csv', header=0, encoding='utf-8')\n",
    "ratings_df = pd.read_csv('data/Ratings.csv', header=0, encoding='utf-8')\n",
    "\n",
    "# creating new columns and intializing each value to 0\n",
    "discussions_df['EnergyAvg'] = 0\n",
    "discussions_df['ValenceAvg'] = 0\n",
    "discussions_df['AcousticnessAvg'] = 0\n",
    "discussions_df['InstrumentalnessAvg'] = 0\n",
    "\n",
    "display(discussions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in range(len(discussions_df)):\n",
    "    results = spotify.album_tracks(discussions_df['SpotifyID'][ind]) # getting tracks from current album via SpotiPy\n",
    "\n",
    "    # filtering track ids into list\n",
    "    ids = []\n",
    "    for track in results['items']:\n",
    "        ids.append(track['id'])\n",
    "    numTracks = len(results['items'])\n",
    "\n",
    "    # getting features from each track in ids via SpotiPy\n",
    "    features = spotify.audio_features(ids)\n",
    "    for feature in features['audio_features']:\n",
    "        # calculating and storing average in df\n",
    "        discussions_df['EnergyAvg'][ind] += (feature['energy'] / numTracks)\n",
    "        discussions_df['ValenceAvg'][ind] += (feature['valence'] / numTracks)\n",
    "        discussions_df['AcousticnessAvg'][ind] += (feature['acousticness'] / numTracks)\n",
    "        discussions_df['InstrumentalnessAvg'][ind] += (feature['instrumentalness'] / numTracks)\n",
    "    time.sleep(3) # sleep with arbitrary time of 3 seconds for circumventing rate limit\n",
    "    \n",
    "display(discussions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discussions_df.to_csv(\"data/Discussions_Audio_Features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discussionsAF_df = pd.read_csv('data/Discussions_Audio_Features.csv', header=0, encoding='utf-8')\n",
    "\n",
    "ratings_df['WeightedEnergyRating'] = 0\n",
    "ratings_df['WeightedValenceRating'] = 0\n",
    "ratings_df['WeightedAcousticnessRating'] = 0\n",
    "ratings_df['WeightedInstrumentalnessRating'] = 0\n",
    "\n",
    "display(ratings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(discussionsAF_df, ratings_df, on='DiscussionID')\n",
    "merged_df = merged_df.drop(['AlbumName', 'ArtistName', 'Date', 'AvgRating', 'Stdev', 'Attendance', 'RotationGenre', 'OtherGenre','Subgenres','ReleaseYear','FavoriteTrack','Popularity','Tracks','SpotifyID','Image', 'Unnamed: 0', 'FavoriteTrack1', 'FavoriteTrack2', 'FavoriteTrack3'], axis='columns') #cleaning up df\n",
    "\n",
    "merged_df['WeightedEnergyRating'] = merged_df['Rating'] * merged_df['EnergyAvg']\n",
    "merged_df['WeightedValenceRating'] = merged_df['Rating'] * merged_df['ValenceAvg']\n",
    "merged_df['WeightedAcousticnessRating'] = merged_df['Rating'] * merged_df['AcousticnessAvg']\n",
    "merged_df['WeightedInstrumentalnessRating'] = merged_df['Rating'] * merged_df['InstrumentalnessAvg']\n",
    "display(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Estimate with weighted ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard mu + b_i + b_u\n",
    "for AF in ['WeightedEnergyRating', 'WeightedValenceRating', 'WeightedAcousticnessRating', 'WeightedInstrumentalnessRating']:\n",
    "    X = merged_df.drop(columns=[AF])\n",
    "    y = merged_df[AF]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    X_train[AF] = y_train\n",
    "\n",
    "    mu = np.mean(X_train[AF])\n",
    "    y_pred = []\n",
    "    for idx, row in X_test.iterrows():\n",
    "        b_u = np.mean(X_train[X_train['MemberID'] == row['MemberID']][AF]) - mu\n",
    "        b_u = 0 if math.isnan(b_u) else b_u\n",
    "        b_i = np.mean(X_train[X_train['DiscussionID'] == row['DiscussionID']][AF]) - mu\n",
    "        b_i = 0 if math.isnan(b_i) else b_i\n",
    "        estimate = max(min(int(np.round(mu - b_u - b_i)), 10), 1)\n",
    "        y_pred.append(estimate)\n",
    "    y_pred = np.array(y_pred)\n",
    "    rmse = np.sqrt(np.mean((y_pred - y_test)**2))\n",
    "    print(AF + \" rmse: \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User-User Collaborative Filtering with weighted Audio Feature Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    retValue = dot_product / (norm_vec1 * norm_vec2) if not math.isnan(dot_product / (norm_vec1 * norm_vec2)) else 0\n",
    "    return retValue\n",
    "\n",
    "def count_non_zeros(arr):\n",
    "    return sum(1 for num in arr if num != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "\n",
    "X = merged_df.drop(columns=['WeightedInstrumentalnessRating'])\n",
    "y = merged_df['WeightedInstrumentalnessRating']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "X_train['WeightedInstrumentalnessRating'] = y_train\n",
    "\n",
    "pivot_df = X_train.pivot(index='DiscussionID', columns='MemberID', values='WeightedInstrumentalnessRating')\n",
    "pivot_df = pivot_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 341\n",
    "album = 68\n",
    "user_sims = {}\n",
    "\n",
    "for member_id in pivot_df.columns:\n",
    "    if member_id != user:\n",
    "        print('User:', member_id)\n",
    "        print('Cosine Sim:', cosine_similarity(pivot_df[user], pivot_df[member_id]))\n",
    "        user_sims[member_id] = cosine_similarity(pivot_df[user], pivot_df[member_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zg/nhdtkkv14fj5w_ftm8sjrdt00000gn/T/ipykernel_3300/3092099898.py:5: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  retValue = dot_product / (norm_vec1 * norm_vec2) if not math.isnan(dot_product / (norm_vec1 * norm_vec2)) else 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.236919177526273\n"
     ]
    }
   ],
   "source": [
    "for idx, row in X_test.iterrows():\n",
    "    user = row['MemberID']\n",
    "    album = row['DiscussionID']\n",
    "    user_sims = {}\n",
    "    \n",
    "    if user not in pivot_df.columns:\n",
    "        y_pred.append(int(np.round(X_train['WeightedInstrumentalnessRating'].mean(),0)))\n",
    "        continue\n",
    "    if album not in pivot_df.index:\n",
    "        y_pred.append(int(np.round(X_train['WeightedInstrumentalnessRating'].mean(),0)))\n",
    "        continue\n",
    "\n",
    "    for member_id in pivot_df.columns:\n",
    "        if member_id != user:\n",
    "            user_sims[member_id] = cosine_similarity(pivot_df[user], pivot_df[member_id])\n",
    "\n",
    "    rated_users = []\n",
    "    for i in pivot_df.columns:\n",
    "        if pivot_df[i][album] != 0 and i != user:\n",
    "            rated_users.append(i)\n",
    "    rated_user_sims = []\n",
    "    for u in rated_users:\n",
    "        rated_user_sims.append(user_sims[u])\n",
    "\n",
    "    top5_rated_users = []\n",
    "    top5_sims = []\n",
    "    sorted_pairs = sorted(zip(rated_users, rated_user_sims), key=lambda x: x[1], reverse=True)\n",
    "    for top5_user, sim in sorted_pairs[:5]:\n",
    "        top5_rated_users.append(top5_user)\n",
    "        top5_sims.append(sim)\n",
    "    normalized_top5_sims = [x * sum(top5_sims) for x in normalized_top5_sims]\n",
    "\n",
    "    pred_r = 0\n",
    "    ind = 0\n",
    "    # # predict the rating with the weighted avg\n",
    "    for u in top5_rated_users:\n",
    "        pred_r += normalized_top5_sims[ind] * pivot_df[u][album]\n",
    "        ind += 1\n",
    "\n",
    "    y_pred.append(int(np.round(pred_r,0)))\n",
    "    \n",
    "y_pred = np.array(y_pred)\n",
    "rmse = np.sqrt(np.mean((y_pred - y_test)**2))\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn\n",
    "Fill out the the database with audio features (watch out for 429 errors!) and run some of the filtering techniques we've covered on it.\n",
    "\n",
    "Content filtering is a great fit here\n",
    "\n",
    "also be cognisant of the fact that audio features are numerical values, and thus can be leveraged in weighted sums, numerical analysis, and any number of data science techniques.\n",
    "\n",
    "and like mentioned in the content filtering slides, feel free to go above and beyond with any external resoruces (like RateYourMusic, AlbumOfTheYear, etc.) you'd like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: integrate Spotify with your filtering code\n",
    "discussions_df['energy'] = 0\n",
    "for ind in range(len(discussions_df)):\n",
    "    try:\n",
    "        results = spotify.album_tracks(discussions_df['SpotifyID'][ind]) # getting tracks from current album via SpotiPy\n",
    "\n",
    "        # filtering track ids into list\n",
    "        ids = []\n",
    "        for track in results['items']:\n",
    "            ids.append(track['id'])\n",
    "        numTracks = len(ids)\n",
    "        features = spotify.audio_features(ids)\n",
    "        for feature in features:\n",
    "            # calculating and storing average in df\n",
    "        \n",
    "            discussions_df['energy'][ind] += (feature['speechiness'] + feature['Rating'])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "lengthy = discussions_df[['DiscussionID','AlbumName', 'energy', 'AvgRating']]\n",
    "\n",
    "ratings_df['energy'] = lengthy['energy']\n",
    "\n",
    "X = ratings_df.drop(columns=['energy'])\n",
    "y = ratings_df['energy']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "X_train['energy'] = y_train\n",
    "\n",
    "merged_df = pd.merge(lengthy, X_train, on='DiscussionID')\n",
    "\n",
    "display(merged_df[[\"DiscussionID\", \"AlbumName\", \"energy_x\", \"Rating\"]])\n",
    "y_pred = []\n",
    "for idx, row in X_test.iterrows():\n",
    "    # Get the decade of the current album\n",
    "    album_decade = discussions_df.loc[discussions_df['DiscussionID'] == row['DiscussionID'], 'energy'].iloc[0]\n",
    "    \n",
    "    # Filter merged_df to only include discussions attended by the user and in the same decade\n",
    "    smaller_df = merged_df[(merged_df['MemberID'] == row['MemberID'])]\n",
    "    \n",
    "    # Calculate the average rating for discussions in the same decade\n",
    "    try:\n",
    "        avg_rating = smaller_df['energy_y'].mean() if not smaller_df.empty else X_train['energy_y'].mean()\n",
    "    except:\n",
    "        pass\n",
    "    y_pred.append(float(np.round(avg_rating, 0)))\n",
    "\n",
    "# Calculate RMSE\n",
    "y_pred = np.array(y_pred)\n",
    "rmse = np.sqrt(np.mean((y_pred - y_test)**2))\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
